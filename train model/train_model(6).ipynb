{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5ac3d7-af71-401f-8c87-7e0b39fa6016",
   "metadata": {},
   "source": [
    "**DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83de459c-10da-44d3-a727-aa168966a971",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Extracted features:  6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pickle import load, dump\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.layers import Add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_features(directory):\n",
    "    model = VGG16()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    print(model.summary())\n",
    "\n",
    "    features = {}\n",
    "    for name in os.listdir(directory):\n",
    "        filename = directory + '\\\\' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        features[image_id] = feature\n",
    "        # print('>', name)\n",
    "    return features\n",
    "\n",
    "\n",
    "dataset_img = \"..\\\\train data\\\\images(6)\"\n",
    "features = extract_features(dataset_img)\n",
    "print('Extracted features: ', len(features))\n",
    "dump(features, open('features(6).p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8acb268-eff8-4347-ba8f-08dd3f1dfbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "dataset_image_path = '..\\\\train data\\\\'\n",
    "doc = load_doc(dataset_image_path + \"captions(6).txt\")\n",
    "# print(doc.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c69d4e-bc76-4bfc-a372-b1ca1075a776",
   "metadata": {},
   "source": [
    "*(Необязательно)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64d356a5-0764-4592-bdf8-a9551eb4af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_imgIDs(doc, filename, start, end):\n",
    "    imgIDs_list = []\n",
    "    for line in doc.split('\\n')[start:end]:\n",
    "        imgID = line.split(\",\")[0]\n",
    "        imgIDs_list.append(imgID)\n",
    "    imgIDs = \"\\n\".join(imgIDs_list)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(imgIDs)\n",
    "    \n",
    "dump_imgIDs(doc, \"trainImages(6).txt\", 0, 15)\n",
    "dump_imgIDs(doc, \"testImages(6).txt\", 15, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f121505c-92ab-4247-b92d-87c72684c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  6\n"
     ]
    }
   ],
   "source": [
    "def load_descriptions(doc):\n",
    "    mapping = {}\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split('.')\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        image_filename, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_filename.split('.')[0]\n",
    "        image_desc = ''.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = []\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: ', len(descriptions))\n",
    "# print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba0729f3-1b76-4aaa-a032-25a0d35b5971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, cap in descriptions.items():\n",
    "        for num_part, caption_part in enumerate(cap):\n",
    "\n",
    "            caption_part.replace('-', ' ')\n",
    "            desc = caption_part.split()\n",
    "\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            desc = [word for word in desc if(len(word) > 1)]\n",
    "            desc = [word for word in desc if(word.isalpha())]\n",
    "\n",
    "            caption_part = ' '.join(desc)\n",
    "            descriptions[img][num_part] = caption_part\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "descriptions = clean_descriptions(descriptions)\n",
    "# print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9aa7a125-3089-4d69-ad25-c6157c03f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 118\n"
     ]
    }
   ],
   "source": [
    "def to_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for value in descriptions.values():\n",
    "        [all_desc.update(d.split()) for d in value]\n",
    "    return all_desc\n",
    "\n",
    "vocab = to_vocabulary(descriptions)\n",
    "# print(vocab)\n",
    "print('Vocabulary size:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4498f899-58fc-48fa-b22b-6849a7017fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines = []\n",
    "    for image_id, image_desc in descriptions.items():\n",
    "        for desc in image_desc:\n",
    "            lines.append(image_id + '\\t' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "save_descriptions(descriptions, 'descriptions(6).txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543cd99-18fa-443e-b785-5d20711a4e22",
   "metadata": {},
   "source": [
    "**DEVELOPING DEEP LEARNING MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8aca41-6e1b-4897-b82b-d3fc64d1c605",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f47fb0c4-e78d-4716-9755-88c2ce4f3de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images loaded: 3\n",
      "{'1000268201_693b08cb0e', '1001773457_577c3a7d70', '1002674143_1b742ab4b8'}\n"
     ]
    }
   ],
   "source": [
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = []\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# trainImages = dataset + 'trainImages(test).txt'\n",
    "train = load_set('trainImages(6).txt')\n",
    "print('Training images loaded:', len(train))\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "502db879-7359-4255-b3e9-458e70be7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training descriptions loaded: 3\n"
     ]
    }
   ],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split('\\t')\n",
    "        image_filename, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_filename.split('.')[0]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = []\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "train_descriptions = load_clean_descriptions('descriptions(6).txt', train)\n",
    "print(\"Training descriptions loaded:\", len(train_descriptions))\n",
    "# print(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "242e9c3e-24e2-4977-86af-13659908b4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded: 3\n"
     ]
    }
   ],
   "source": [
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "train_features = load_photo_features('features(6).p', train)\n",
    "print('Features loaded:', len(train_features.keys()))\n",
    "# for i, feature in enumerate(features.values()):\n",
    "#     print(f'@{i} - {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eda0bfcf-29c2-4c33-8ecc-a705363d23d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 76\n"
     ]
    }
   ],
   "source": [
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for values in descriptions.values():\n",
    "        [all_desc.append(x) for x in values]\n",
    "    return all_desc\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size:', vocab_size)\n",
    "dump(tokenizer, open('tokenizer(6).p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5ce95-346b-49cf-8e2e-d4db9f3f5638",
   "metadata": {},
   "source": [
    "*Вариант 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3bacb86f-d871-4ec5-8a57-b6bbd0ef5687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 20\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "    x1, x2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                x1.append(photos[key][0])\n",
    "                x2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(x1), array(x2), array(y)\n",
    "\n",
    "def max_length(descriptions):\n",
    "    lines = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Maximum sentence length:', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d1341-9df2-4d8b-892f-0bee62e3c0d6",
   "metadata": {},
   "source": [
    "*Вариант 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "437014c1-a811-4cfd-b4b2-3b832ae274ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 20\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    x1, x2, y = [], [], []\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            input_seq, output_seq = seq[:i], seq[i]\n",
    "            input_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
    "            output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "            x1.append(photo)\n",
    "            x2.append(input_seq)\n",
    "            y.append(output_seq)\n",
    "    return np.array(x1), np.array(x2), np.array(y)\n",
    "\n",
    "def max_length(descriptions):\n",
    "    lines = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Maximum sentence length:', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289b9e1-43f7-4f9c-b5c0-fcae19682582",
   "metadata": {},
   "source": [
    "**Defining the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53a5f16f-7fc6-4737-9efc-1ac26815716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    # Извлечение особенностей\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # Процессор последовательностей\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # Декодер\n",
    "    decoder1 = Add()([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # Связывание всего в единое\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # Подведение итогов\n",
    "    print(model.summary())\n",
    "    # plot_model(model, to_file='models/model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96440a0-0e3b-431f-85ef-312aaed7a223",
   "metadata": {},
   "source": [
    "**Fitting the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "419a28a5-ec58-48c2-8f01-56c8c30e012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1train, x2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
    "\n",
    "# filename = 'testImages(6).txt'\n",
    "# test = load_set(filename)\n",
    "# print('Dataset:', len(test))\n",
    "# test_descriptions = load_clean_descriptions('descriptions(6).txt', test)\n",
    "# print('Descriptions: test=', len(test_descriptions))\n",
    "# test_features = load_photo_features('features(6).p', test)\n",
    "# print('Photos: test=', len(test_features))\n",
    "\n",
    "# x1test, x2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "83113423-a34b-4fd2-bfd1-b2a4e4b60aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 4096)\n",
      "(52, 20)\n",
      "(52, 76)\n"
     ]
    }
   ],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word\n",
    "\n",
    "generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cfaa20f5-5bb4-4ede-ba05-18b51372d0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 20, 256)      19456       ['input_16[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096)         0           ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 20, 256)      0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          1048832     ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 256)          525312      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 256)          0           ['dense_5[0][0]',                \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          65792       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 76)           19532       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,678,924\n",
      "Trainable params: 1,678,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "3/3 [==============================] - 11s 797ms/step - loss: 5.4694\n",
      "3/3 [==============================] - 3s 932ms/step - loss: 4.4618\n",
      "3/3 [==============================] - 3s 816ms/step - loss: 4.1926\n",
      "3/3 [==============================] - 3s 877ms/step - loss: 3.9520\n",
      "3/3 [==============================] - 2s 692ms/step - loss: 3.7579\n",
      "3/3 [==============================] - 2s 758ms/step - loss: 3.7017\n",
      "3/3 [==============================] - 3s 917ms/step - loss: 3.6367\n",
      "3/3 [==============================] - 2s 751ms/step - loss: 3.5134\n",
      "3/3 [==============================] - 2s 792ms/step - loss: 3.5193\n",
      "3/3 [==============================] - 2s 611ms/step - loss: 3.3768\n",
      "3/3 [==============================] - 2s 776ms/step - loss: 3.2671\n",
      "3/3 [==============================] - 2s 677ms/step - loss: 3.2197\n",
      "3/3 [==============================] - 2s 623ms/step - loss: 3.1013\n",
      "3/3 [==============================] - 2s 787ms/step - loss: 3.0586\n",
      "3/3 [==============================] - 3s 771ms/step - loss: 2.9653\n",
      "3/3 [==============================] - 2s 818ms/step - loss: 2.9091\n",
      "3/3 [==============================] - 2s 690ms/step - loss: 2.8076\n",
      "3/3 [==============================] - 2s 728ms/step - loss: 2.7849\n",
      "3/3 [==============================] - 2s 649ms/step - loss: 2.6931\n",
      "3/3 [==============================] - 3s 778ms/step - loss: 2.6697\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    generator =  data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "    history = model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    \n",
    "    # plt.plot(history.history['accuracy'])\n",
    "    # plt.plot(history.history['val_accuracy'])\n",
    "    # plt.title('model accuracy')\n",
    "    # plt.ylabel('accuracy')\n",
    "    # plt.xlabel('epoch')\n",
    "    # plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.show()\n",
    "    # # summarize history for loss\n",
    "    # plt.plot(history.history['loss'])\n",
    "    # plt.plot(history.history['val_loss'])\n",
    "    # plt.title('model loss')\n",
    "    # plt.ylabel('loss')\n",
    "    # plt.xlabel('epoch')\n",
    "    # plt.legend(['train', 'test'], loc='upper left')\n",
    "    # plt.show()\n",
    "    \n",
    "    model.save('models(6)/model_' + str(i+1) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0a9e0646-209f-4e84-837c-6e06dd4425cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    input_text  = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        seq = pad_sequences([seq], maxlen=max_length)\n",
    "        yhat = model.predict([photo, seq], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        input_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bdc26d6c-5f73-4b8a-b8f1-13f5b4deb8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 3\n",
      "Descriptions: test= 3\n",
      "Photos: test= 3\n",
      "BLEU-1: 0.3807936770679397\n",
      "BLEU-2: 0.11687506392880498\n",
      "BLEU-3: 1.2924959128760296e-93\n",
      "BLEU-4: 4.632058777330975e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "E:\\anaconda3\\envs\\tf\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from pickle import load\n",
    "\n",
    "\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = [], []\n",
    "    for key, desc_list in descriptions.items():\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1:', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2:', corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3:', corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4:', corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "testImages = 'testImages(6).txt'\n",
    "test = load_set(testImages)\n",
    "print('Dataset:', len(test))\n",
    "test_descriptions = load_clean_descriptions('descriptions(6).txt', test)\n",
    "print('Descriptions: test=', len(test_descriptions))\n",
    "test_features = load_photo_features('features(6).p', test)\n",
    "print('Photos: test=', len(test_features))\n",
    "tokenizer = load(open('tokenizer(6).p', 'rb'))\n",
    "model = load_model('models(6)/model_20.h5')\n",
    "max_length = 20\n",
    "\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
